{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from array import array\n",
    "import struct\n",
    "import cv2 \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from one_layer_nn import OneLayerNN\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_data(path):\n",
    "    data_dir = Path(\"./data\")\n",
    "    with open(data_dir / path, \"rb\") as f:\n",
    "        # IDX file format\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        image_data = array(\"B\", f.read())   \n",
    "    images = []\n",
    "    for i in range(size):\n",
    "        image = np.array(image_data[i * rows * cols:(i + 1) * rows * cols]).reshape(28, 28)\n",
    "        images.append(image)\n",
    "    return np.array(images)\n",
    "\n",
    "\n",
    "def read_labels(path):\n",
    "    data_dir = Path(\"./data\")\n",
    "    with open(data_dir / path, \"rb\") as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        if magic != 2049:\n",
    "            raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "        labels = np.array(array(\"B\", f.read()))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "train_val_labels = read_labels(\"train-labels.idx1-ubyte\")\n",
    "train_val_images = read_image_data(\"train-images.idx3-ubyte\")\n",
    "test_images = read_image_data(\"t10k-images.idx3-ubyte\")\n",
    "test_labels = read_labels(\"t10k-labels.idx1-ubyte\")\n",
    "print(train_val_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc33fe54d00>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show image\n",
    "\n",
    "plt.imshow(test_images[10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(label):\n",
    "    encoded_label = np.zeros(10)\n",
    "    encoded_label[label] = 1\n",
    "    return encoded_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set trainning dataset\n",
    "# Set validate dataset\n",
    "\n",
    "train_images = train_val_images[:int(0.9 * len(train_val_images))]\n",
    "val_images = train_val_images[len(train_images):]\n",
    "\n",
    "train_labels = train_val_labels[:len(train_images)]\n",
    "val_labels = train_val_labels[len(train_images):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_layer_nn = OneLayerNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25189/4021182234.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  torch_train_labels = torch.Tensor([onehot(label) for label in train_labels])\n"
     ]
    }
   ],
   "source": [
    "torch_train_images = torch.Tensor(train_images).reshape(-1, 28 * 28)\n",
    "torch_val_images = torch.Tensor(val_images).reshape(-1, 28 * 28)\n",
    "torch_test_images = torch.Tensor(test_images).reshape(-1, 28 * 28)\n",
    "torch_train_labels = torch.Tensor([onehot(label) for label in train_labels])\n",
    "torch_val_labels = torch.Tensor([onehot(label) for label in val_labels])\n",
    "torch_test_labels = torch.Tensor([onehot(label) for label in test_labels])\n",
    "torch_train_dataset = torch.hstack((torch_train_labels, torch_train_images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(params=one_layer_nn.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss 0.20985274016857147\n",
      "Accuracy 0.9393333333333334\n",
      "Val Loss 0.21142853796482086\n",
      "Accuracy 0.944\n",
      "Val Loss 0.19662968814373016\n",
      "Accuracy 0.9566666666666667\n",
      "Val Loss 0.18643616139888763\n",
      "Accuracy 0.9593333333333334\n",
      "Val Loss 0.1822238713502884\n",
      "Accuracy 0.9603333333333334\n",
      "Val Loss 0.18014958500862122\n",
      "Accuracy 0.9583333333333334\n",
      "Val Loss 0.17943190038204193\n",
      "Accuracy 0.9613333333333334\n",
      "Val Loss 0.17741777002811432\n",
      "Accuracy 0.9633333333333334\n",
      "Val Loss 0.17686671018600464\n",
      "Accuracy 0.9618333333333333\n",
      "Val Loss 0.17609421908855438\n",
      "Accuracy 0.9658333333333333\n",
      "Val Loss 0.17542989552021027\n",
      "Accuracy 0.9663333333333334\n",
      "Val Loss 0.17829130589962006\n",
      "Accuracy 0.9618333333333333\n",
      "Val Loss 0.17949818074703217\n",
      "Accuracy 0.9628333333333333\n",
      "Val Loss 0.18042023479938507\n",
      "Accuracy 0.9681666666666666\n",
      "Val Loss 0.18189674615859985\n",
      "Accuracy 0.9655\n",
      "Val Loss 0.1840190589427948\n",
      "Accuracy 0.9636666666666667\n",
      "Val Loss 0.18626224994659424\n",
      "Accuracy 0.9661666666666666\n",
      "Val Loss 0.1872359961271286\n",
      "Accuracy 0.9693333333333334\n",
      "Val Loss 0.19042493402957916\n",
      "Accuracy 0.9658333333333333\n",
      "Val Loss 0.19210045039653778\n",
      "Accuracy 0.9666666666666667\n",
      "Val Loss 0.19588486850261688\n",
      "Accuracy 0.9643333333333334\n",
      "Val Loss 0.19803868234157562\n",
      "Accuracy 0.9681666666666666\n",
      "Val Loss 0.20105770230293274\n",
      "Accuracy 0.9668333333333333\n",
      "Val Loss 0.20412953197956085\n",
      "Accuracy 0.9623333333333334\n",
      "Val Loss 0.20770390331745148\n",
      "Accuracy 0.9656666666666667\n",
      "Val Loss 0.20921172201633453\n",
      "Accuracy 0.9698333333333333\n",
      "Val Loss 0.21242457628250122\n",
      "Accuracy 0.9661666666666666\n",
      "Val Loss 0.21599960327148438\n",
      "Accuracy 0.9633333333333334\n",
      "Val Loss 0.21948379278182983\n",
      "Accuracy 0.963\n",
      "Val Loss 0.22407689690589905\n",
      "Accuracy 0.9645\n"
     ]
    }
   ],
   "source": [
    "one_layer_nn = OneLayerNN()\n",
    "optim = torch.optim.SGD(params=one_layer_nn.parameters(), lr=1e-3)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "epochs = 30\n",
    "batch_size = 10\n",
    "val_predictions = []\n",
    "for epoch in range(epochs):\n",
    "    shuffled_order = torch.randperm(len(torch_train_dataset))\n",
    "    val_predictions = []\n",
    "    for index in range(len(shuffled_order) // batch_size):\n",
    "        batch = torch_train_dataset[shuffled_order[index * batch_size:(index + 1) * batch_size]]\n",
    "        y_true = batch[:,:10]\n",
    "        y_pred = one_layer_nn(batch[:,10:]) # What's happen?\n",
    "        loss = torch.nn.functional.cross_entropy(y_pred, y_true) # What's happen?\n",
    "        optim.zero_grad() # Chuyen grad ve 0\n",
    "        loss.backward() # Backward \n",
    "        optim.step()\n",
    "        train_losses.append(loss.detach())\n",
    "    with torch.no_grad():\n",
    "        for batch in range(len(torch_val_images) // batch_size):\n",
    "            y_pred = one_layer_nn(torch_val_images[batch * batch_size:(batch + 1) * batch_size])\n",
    "            y_true = torch_val_labels[batch * batch_size:(batch + 1) * batch_size]\n",
    "            loss = torch.nn.functional.cross_entropy(y_pred, y_true)\n",
    "            val_losses.append(loss)\n",
    "            val_predictions.extend(torch.argmax(y_pred, axis=1))\n",
    "        print(\"Val Loss\", (sum(val_losses) / len(val_losses)).item())\n",
    "        print(\"Accuracy\", sum(np.array(val_predictions) == val_labels) / len(val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(one_layer_nn.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneLayerNN()\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "def convert_img_to_mnist(path):\n",
    "    image = Image.open(path)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    image = image.convert('L')\n",
    "\n",
    "    # Resize the image to 28x28 pixels\n",
    "    image = image.resize((28, 28))\n",
    "\n",
    "    # Convert the image to a NumPy array\n",
    "    image_array = np.array(image)\n",
    "    # Ensure pixel values are within the range [0, 255]\n",
    "    image_array = (image_array / 255.0) * 255\n",
    "    image_array = torch.Tensor(image_array).reshape(-1, 28 * 28)\n",
    "    test_predictions = []\n",
    "    with torch.no_grad():\n",
    "        y_pred = one_layer_nn(image_array[:])\n",
    "        test_predictions.extend(torch.argmax(y_pred, axis=1))\n",
    "        print(test_predictions)\n",
    "    test_predictions = torch.Tensor(test_predictions).numpy().astype(np.uint8)\n",
    "    print(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneLayerNN()\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "def convert_img_to_mnist_2(path):\n",
    "    image = Image.open(path)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    image = image.convert('L')\n",
    "\n",
    "    # Resize the image to 28x28 pixels\n",
    "    image = image.resize((28, 28))\n",
    "\n",
    "    # Convert the image to a NumPy array\n",
    "    image_array = np.array(image)\n",
    "    # Ensure pixel values are within the range [0, 255]\n",
    "    image_array = (image_array / 255.0) * 255\n",
    "    image_array = torch.Tensor(image_array).reshape(-1, 28 * 28)\n",
    "    test_predictions = []\n",
    "    y_pred = one_layer_nn(image_array[:])\n",
    "    max_index = np.argmax(y_pred.detach().numpy())\n",
    "    print(max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "image_array = convert_img_to_mnist_2('out.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
